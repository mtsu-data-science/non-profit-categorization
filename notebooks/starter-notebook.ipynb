{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# all notebooks need to run from the root directory\n",
    "# this will check to see if the current working directory is notebooks, if so, it will change\n",
    "# to root directory.  if already in the root directory, will not modify\n",
    "if \"notebooks\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from src.utility.get_data import get_non_profit_text_df, get_non_profit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nonprofit.txt...\n",
      "Downloading nonprofit_text.txt...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nonprofit_text_id</th>\n",
       "      <th>reporting_year</th>\n",
       "      <th>nonprofit_id</th>\n",
       "      <th>grouptype</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2020</td>\n",
       "      <td>4553</td>\n",
       "      <td>charitablegroup</td>\n",
       "      <td>MAINTAIN AND BEAUTIFY THE DEGREGORIE PARK MAIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>4978</td>\n",
       "      <td>charitablegroup</td>\n",
       "      <td>PROVIDING HOUSING AND RESIDENTIAL SERVICES FOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>37</td>\n",
       "      <td>charitablegroup</td>\n",
       "      <td>PROVIDING SCHOLARSHIPS AND EDUCATIONS ASSISTAN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nonprofit_text_id  reporting_year  nonprofit_id        grouptype  \\\n",
       "0                 10            2020          4553  charitablegroup   \n",
       "1                 11            2019          4978  charitablegroup   \n",
       "2                 12            2017            37  charitablegroup   \n",
       "\n",
       "                                         description  \n",
       "0  MAINTAIN AND BEAUTIFY THE DEGREGORIE PARK MAIN...  \n",
       "1  PROVIDING HOUSING AND RESIDENTIAL SERVICES FOR...  \n",
       "2  PROVIDING SCHOLARSHIPS AND EDUCATIONS ASSISTAN...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = get_non_profit_text_df()\n",
    "df_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nonprofit_id</th>\n",
       "      <th>reporting_year</th>\n",
       "      <th>ein</th>\n",
       "      <th>businessname</th>\n",
       "      <th>phone</th>\n",
       "      <th>address1</th>\n",
       "      <th>address2</th>\n",
       "      <th>city</th>\n",
       "      <th>stabbrv</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>10274998</td>\n",
       "      <td>MOUNT ST JOSEPH</td>\n",
       "      <td>2078730705</td>\n",
       "      <td>7 HIGHWOOD STREET</td>\n",
       "      <td>7 HIGHWOOD STREET</td>\n",
       "      <td>WATERVILLE</td>\n",
       "      <td>ME</td>\n",
       "      <td>04901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>10275026</td>\n",
       "      <td>BELFAST CURLING CLUB</td>\n",
       "      <td>2073389851</td>\n",
       "      <td>PO BOX 281 BELMONT AVE</td>\n",
       "      <td>PO BOX 281 BELMONT AVE</td>\n",
       "      <td>BELFAST</td>\n",
       "      <td>ME</td>\n",
       "      <td>04915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>10275130</td>\n",
       "      <td>Unity College</td>\n",
       "      <td>2075097100</td>\n",
       "      <td>90 Quaker Hill Road</td>\n",
       "      <td>90 Quaker Hill Road</td>\n",
       "      <td>Unity</td>\n",
       "      <td>ME</td>\n",
       "      <td>04988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nonprofit_id  reporting_year       ein           businessname       phone  \\\n",
       "0            10            2021  10274998       MOUNT ST JOSEPH   2078730705   \n",
       "1            11            2020  10275026  BELFAST CURLING CLUB   2073389851   \n",
       "2            12            2021  10275130         Unity College   2075097100   \n",
       "\n",
       "                 address1                address2        city stabbrv    zip  \n",
       "0       7 HIGHWOOD STREET       7 HIGHWOOD STREET  WATERVILLE      ME  04901  \n",
       "1  PO BOX 281 BELMONT AVE  PO BOX 281 BELMONT AVE     BELFAST      ME  04915  \n",
       "2     90 Quaker Hill Road     90 Quaker Hill Road       Unity      ME  04988  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np = get_non_profit_df()\n",
    "df_np.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Given the above data set of non-profit organizations, we will be using natural language processing to categorize the non-profit organizations based on their tax description. We do not have labels for the non-profit organizations, so this will be an unsupervised learning problem. Let's start with the simplest possible approach, let's get our words into an embedding space and run some simple clustering to see what comes out. \n",
    "\n",
    "Since we don't have labels we won't be able to quantitatively evaluate the results but we can take a qualitative look and a quick look at possible next steps. \n",
    "\n",
    "We'll be using some of the tools from [this article](https://maxhalford.github.io/blog/unsupervised-text-classification/) as well as filling in the rest with some Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing punctuation and newline characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())  # remove multiple whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 777.4 MB 8.2 kB/s  eta 0:00:013     |███████████████████████████████▋| 767.3 MB 6.1 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from en-core-web-lg==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.22.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: setuptools in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (58.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Users/rharrigan/.pyenv/versions/3.10.2/envs/npc/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Download spacy word embeddings from Word2Vec\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded document:\n",
      "MAINTAIN AND BEAUTIFY THE DEGREGORIE PARK MAINTAIN AND BEAUTIFY THE SHORE PATH INSTALLED HISTORICAL SIGNAGE MAINTAIN AND BEAUTIFY THE HOWE MEMORIAL PARK\n",
      "\n",
      "output embedding is (300,)\n",
      "[ 0.38682842 -0.075748   -0.05325746 -0.16872934  0.06439866 -0.06584267\n",
      " -0.15967049  0.08441201 -0.38315043  1.7192727 ]\n"
     ]
    }
   ],
   "source": [
    "# Now load our embeddings and setup an embed function\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def embed(tokens, nlp):\n",
    "    \"\"\"Return the centroid of the embeddings for the given tokens.\n",
    "\n",
    "    Out-of-vocabulary tokens are cast aside. Stop words are also\n",
    "    discarded. An array of 0s is returned if none of the tokens\n",
    "    are valid.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lexemes = (nlp.vocab[token] for token in tokens)\n",
    "\n",
    "    vectors = np.asarray([\n",
    "        lexeme.vector\n",
    "        for lexeme in lexemes\n",
    "        if lexeme.has_vector\n",
    "        and not lexeme.is_stop\n",
    "        and len(lexeme.text) > 1\n",
    "    ])\n",
    "\n",
    "    if len(vectors) > 0:\n",
    "        centroid = vectors.mean(axis=0)\n",
    "    else:\n",
    "        width = nlp.meta['vectors']['width']  # typically 300\n",
    "        centroid = np.zeros(width)\n",
    "\n",
    "    return centroid\n",
    "doc = df_text.loc[0, \"description\"]\n",
    "tokens = doc.split(' ')\n",
    "centroid = embed(tokens, nlp)\n",
    "\n",
    "print(\"Embedded document:\")\n",
    "print(doc)\n",
    "print()\n",
    "print(f\"output embedding is {centroid.shape}\")\n",
    "print(centroid[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what do we do with embeddings?\n",
    "Well now that we have some embeddings we need to utilize these to classify our documents. But we don't have any classes! We'll need to create some. We could try just making some up but we can do a bit better, what if we just look at the natural clusters which have emerged from the embeddings?\n",
    "\n",
    "We can use nearest neighbors to find the nearest neighbors of our embeddings and then we can use those to create our classes. We'll be using scikit-learn's [neighbors](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors) module. We'll be using Ball Tree for the nearest neighbors since it [performs better in higher dimensional spaces](https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940) and we have 300 dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835559, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some cleanup, some documents have NaN or float descriptions\n",
    "mask = df_text['description'].apply(lambda x: isinstance(x, str))\n",
    "df_text = df_text[mask]\n",
    "df_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a424ee861cf84841bc4e0a102da7e085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1835559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1835559, 300), (1835559, 5))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Note that this takes ~4 minutes on my machine\n",
    "X = []\n",
    "for doc in tqdm(df_text[\"description\"].values):\n",
    "    vals = embed(doc.split(' '), nlp)\n",
    "    X.append(vals)\n",
    "X = np.array(X)\n",
    "X.shape, df_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "# This took ~3 minutes on my machine\n",
    "tree = BallTree(X, leaf_size=30, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input document: TO ESTABLISH ORGANIZED AMATEUR VOLLEYBALL WITH ULTIMATE OBJECTIVES OF SOCIAL, PHYSICAL, MENTAL, AND MORAL DEVELOPMENT OF GIRLS AGED 9 TO 17 YEARS. A PROGRAM OF FRIENDLY COMPETITION WITH THE GOAL OF EDUCATING PLAYERS ABOUT SPORTSMANSHIP, TEAMWORK, FELLOWSHIP, COURTESY, DISCIPLINE, AND INTEGRITY WILL BE ESTABLISHED.\n",
      "\n",
      "Neighbor 351103 is 1.30: Tournaments and games for junior hockey players\n",
      "Neighbor 261062 is 1.31: THE ORGANIZATION PRODUCED 7 SHOWS AND EVENTS DURING THE YEAR, INCLUDING 2 MAINSTAGE PRODUCTIONS, 4 STAGED READINGS, AND 1 FESTIVAL, WITH AN APPROXIMATE TOTAL ATTENDANCE OF 5230.\n",
      "Neighbor 1668434 is 1.36: THE ORGANIZATION WORKS ON ENVIRONMENT AND EDUCATION PROGRAMS.\n"
     ]
    }
   ],
   "source": [
    "# Now get the distance and indices of the 3 nearest neighbors\n",
    "def print_neighbors(tgt_ind, tree, n_neighbors=3):\n",
    "    tgt_vec = X[tgt_ind, :]\n",
    "    tgt_vec = tgt_vec[np.newaxis, ...]\n",
    "    dist, ind = tree.query(tgt_vec, k=n_neighbors + 1)\n",
    "    print(f\"Input document: {df_text.loc[tgt_ind, 'description']}\")\n",
    "    print()\n",
    "    # Skip the first one since it's always itself\n",
    "    for i, d in zip(ind[0].tolist()[1:], dist[0].tolist()[1:]):\n",
    "        print(f\"Neighbor {i:02d} is {d:0.2f}: {df_text.loc[i, 'description']}\")\n",
    "print_neighbors(54624, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "These results show some promise, we can see that we're starting to capture some meaningful elements but there is definitely a lot of work left to do!\n",
    "\n",
    "# Conclusions\n",
    "We've seen how we can start to classify documents using NLP and unsupervised learning methods. Some next steps might include:\n",
    "* Identify the classes we should be using by examining documents that don't fit our single classifier and coming up with more classes. Repeat this process. \n",
    "* Improving the word embedding space or even using a more sophisticated model like sentence embeddings (GloVe or BERT)\n",
    "* Using more sophisticated clustering methods like Hierarchical Clustering\n",
    "\n",
    "\n",
    "For some more sophistication take a look at [this article](https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7319f563673677e422a10613494ec1f4af38d3dabd788e29e91dd57d2c43df3a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
